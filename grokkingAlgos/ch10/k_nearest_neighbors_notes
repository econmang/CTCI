k-Nearest Neighbors:

k-nearest neighbors (KNN) is an algorithm for classification.
|
|
|               o o
|                o   o
|  *       (x)  o  o   o
|    *  *           o
| *    *
|   *
|__________________________

In trying to classify the (x) on the graph above, you can utilize the distance to a set of k neighbors. If it's closer to more o's than *'s, we can assume its classification as an o.


Feature Extraction:
Context is important for the classification of items, objects, users, etc.
For that reason, you can carefully choose any number of features that aid in your process of classification, but collecting an excess of features or under-collection can both lead to classification error.

In terms of distance used, a common metric is the Pythagorean distance formula sqrt((x_2 - x_1)^2 + (y_2 - y_1)^2)
Since you can have any number of features, this can be expanded to include as many features as needed to determine "feature distance"

Regression:
You can find the avg of the output for the k closest neighbors (similar data sources or users, etc.) to the user to find a predicted output/response.
KNN can be used for classification and regression
Classification - Categorization into a group
Regression - Predicting a response (often numeric)

It's important to pick good features for the classifier/regressive algo you are building. Features should correlate directly to the response you are predicting or groups you are classifying.
Features should not be biased.

KNN is an introductory algo into the field of machine learning. Optical Character Recognition and simplistic recommendation engines can use this method under the hood. The process of extracting important features is called training.

Spam Filter:
Spam filters use another algo called naive Bayes classification that has similar application to KNN. After training, it takes in input and finds the probability that the input has a given classification label.
